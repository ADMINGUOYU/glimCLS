#!/usr/bin/env python
"""
Calculates stage 1 text generation metrics (utility script)
-> Please provide dataframe generated by predict_glim_parallel_and_pack.py

"""

PATH_TO_STAGE2_DATAFRAME = './data/zuco_preprocessed_dataframe/stage2.df'
PATH_TO_VARIANTS = './data/zuco_preprocessed_dataframe/zuco_label_8variants.df'

VARIANT_KEYS = \
    ['lexical simplification (v0)', 'lexical simplification (v1)',
    'semantic clarity (v0)', 'semantic clarity (v1)',
    'syntax simplification (v0)', 'syntax simplification (v1)',
    'naive rewritten', 'naive simplified']

DEVICE = "cuda:0"

SET = 'test'

import os
import sys
import torch
import pandas as pd
from tqdm import tqdm
from typing import List, Dict

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import torchmetrics for evaluation (following GLIM approach)
from torchmetrics.functional.text import bleu_score, rouge_score, word_error_rate


def compute_metrics(predictions: List[Dict]) -> Dict:
    """
    Compute evaluation metrics for the predictions.
    
    Following the GLIM approach for computing BLEU, ROUGE, and WER metrics.
    Reference: https://github.com/justin-xzliu/GLIM/blob/main/model/glim.py
    
    Args:
        predictions: List of prediction dictionaries
            | -> "prediction"
            | -> "target"
    Returns:
        Dictionary of metrics including:
            - BLEU-1, BLEU-2, BLEU-3, BLEU-4
            - ROUGE-1 (fmeasure, precision, recall)
            - Word Error Rate (WER)
            - Per-sample metrics list
    """
    # Initialize lists for collecting metrics (following GLIM's cal_gen_metrics)
    bleu1_scores = []
    bleu2_scores = []
    bleu3_scores = []
    bleu4_scores = []
    rouge1_fmeasure = []
    rouge1_precision = []
    rouge1_recall = []
    wer_scores = []
    
    # Per-sample metrics for detailed analysis
    per_sample_metrics = []

    # Load variants table
    var_df : pd.DataFrame = pd.read_pickle(PATH_TO_VARIANTS)
    print(f"Received variants dataframe columns: {var_df.columns.to_list()}")
    
    print("\nComputing metrics...")
    for pred_dict in tqdm(predictions, desc="Computing metrics"):
        pred_text = pred_dict["prediction"]
        target_text = pred_dict["target"]
        
        # Handle empty predictions or targets
        if not pred_text or not pred_text.strip():
            pred_text = " "  # Use space to avoid empty string issues
        if not target_text or not target_text.strip():
            target_text = " "

        # Look up for raw and variants
        index = var_df.index[var_df['input text'] == target_text].to_list()
        assert len(index) > 0, f"[ERROR] Input text {target_text} not found"
        index = index[0]
        raw_text = var_df.loc[index, 'raw text']
        variants = []
        for key in VARIANT_KEYS:
            variants.append(var_df.loc[index, key])
        
        # Compute BLEU scores (n-gram 1-4)
        # Following GLIM:  bleu_score([pred], [targets], n_gram=n)
        # Note: targets can be a tuple/list for multiple references, here we use single reference
        try:
            b1 = bleu_score([pred_text], [variants], n_gram=1)
            b2 = bleu_score([pred_text], [variants], n_gram=2)
            b3 = bleu_score([pred_text], [variants], n_gram=3)
            b4 = bleu_score([pred_text], [variants], n_gram=4)
        except Exception as e:
            # Fallback for edge cases (very short texts, etc.)
            b1 = b2 = b3 = b4 = torch.tensor(0.0)
        
        bleu1_scores.append(b1)
        bleu2_scores.append(b2)
        bleu3_scores.append(b3)
        bleu4_scores.append(b4)
        
        # Compute ROUGE-1 scores
        # Following GLIM: rouge_score([pred], [targets], rouge_keys='rouge1')
        try:
            rouge1_dict = rouge_score([pred_text], [[raw_text]], rouge_keys='rouge1')
            r1_fmeasure = rouge1_dict['rouge1_fmeasure']
            r1_precision = rouge1_dict['rouge1_precision']
            r1_recall = rouge1_dict['rouge1_recall']
        except Exception as e:
            r1_fmeasure = r1_precision = r1_recall = torch.tensor(0.0)
        
        rouge1_fmeasure.append(r1_fmeasure)
        rouge1_precision.append(r1_precision)
        rouge1_recall.append(r1_recall)
        
        # Compute Word Error Rate
        # Following GLIM: word_error_rate([pred], [target])
        try:
            wer = word_error_rate([pred_text], [raw_text])
        except Exception as e:
            wer = torch.tensor(1.0)  # Maximum error for edge cases
        
        wer_scores.append(wer)
        
        # Store per-sample metrics
        per_sample_metrics.append({
            "prediction": pred_text,
            "target": target_text,
            "bleu1": b1.item() if torch.is_tensor(b1) else b1,
            "bleu2": b2.item() if torch.is_tensor(b2) else b2,
            "bleu3": b3.item() if torch.is_tensor(b3) else b3,
            "bleu4": b4.item() if torch.is_tensor(b4) else b4,
            "rouge1_fmeasure": r1_fmeasure.item() if torch.is_tensor(r1_fmeasure) else r1_fmeasure,
            "rouge1_precision": r1_precision.item() if torch.is_tensor(r1_precision) else r1_precision,
            "rouge1_recall": r1_recall.item() if torch.is_tensor(r1_recall) else r1_recall,
            "wer": wer.item() if torch.is_tensor(wer) else wer,
        })
    
    # Compute mean metrics (following GLIM's approach of stacking and averaging)
    metrics_mean = {
        "bleu1": torch.stack(bleu1_scores).mean().item(),
        "bleu2": torch.stack(bleu2_scores).mean().item(),
        "bleu3": torch.stack(bleu3_scores).mean().item(),
        "bleu4": torch.stack(bleu4_scores).mean().item(),
        "rouge1_fmeasure": torch.stack(rouge1_fmeasure).mean().item(),
        "rouge1_precision": torch.stack(rouge1_precision).mean().item(),
        "rouge1_recall": torch.stack(rouge1_recall).mean().item(),
        "wer": torch.stack(wer_scores).mean().item(),
    }
    
    # Print summary metrics
    print("\n" + "=" * 60)
    print("Text Generation Metrics Summary")
    print("=" * 60)
    print(f"BLEU-1:            {metrics_mean['bleu1']:.4f}")
    print(f"BLEU-2:            {metrics_mean['bleu2']:.4f}")
    print(f"BLEU-3:            {metrics_mean['bleu3']:.4f}")
    print(f"BLEU-4:            {metrics_mean['bleu4']:.4f}")
    print("-" * 60)
    print(f"ROUGE-1 F-measure:  {metrics_mean['rouge1_fmeasure']:.4f}")
    print(f"ROUGE-1 Precision: {metrics_mean['rouge1_precision']:.4f}")
    print(f"ROUGE-1 Recall:    {metrics_mean['rouge1_recall']:.4f}")
    print("-" * 60)
    print(f"Word Error Rate:   {metrics_mean['wer']:.4f}")
    print("=" * 60)
    
    return {
        "mean":  metrics_mean,
        "per_sample":  per_sample_metrics,
    }


def compute_retrieval_metrics(
    predictions: List[Dict],
    top_k: List[int] = [1, 5, 10],
    rand_grp: int = 5,
    rand_btsz:int = 24,
    device: str = "cpu"
) -> Dict:
    """
    Compute retrieval metrics using cosine similarity between generated text's embeddings
    and target text embeddings (use raw text here). 
    
    Args:
        predictions: List of prediction dictionaries
            | -> "prediction"
            | -> "target"
        top_k: Top K retrieval to calculate
        rand_grp: How many batchs to fetch in calculation
        rand_btsz: How many samples in each batch
        device: Device to compute on
        
    Returns:
        Dictionary of retrieval metrics (top-1, top-5, top-10 accuracy)
    """

    # Initialize embedding generating models
    from . import generate_embedding
    generate_embedding.setup(device)

    # Load variants table
    var_df : pd.DataFrame = pd.read_pickle(PATH_TO_VARIANTS)
    print(f"Received variants dataframe columns: {var_df.columns.to_list()}")

    # Initialize lists pred_text and target_text
    pred_text = []
    target_text = []

    # Gathering sentences
    print("\nGathering sentences...")
    for pred_dict in tqdm(predictions, desc="Gathering sentences"):
        # Get text
        pred : str = pred_dict["prediction"]
        target : str = pred_dict["target"]

        # Handle empty predictions or targets
        if not pred or not pred.strip():
            pred = " "  # Use space to avoid empty string issues
        if not target or not target.strip():
            target = " "

        # Look up for raw and variants
        index = var_df.index[var_df['input text'] == target].to_list()
        assert len(index) > 0, f"[ERROR] Input text {target} not found"
        index = index[0]
        raw_text = var_df.loc[index, 'raw text']
    
        # Append
        pred_text.append(pred)
        target_text.append(raw_text)
        
    # Calculate embeddings
    print("\nCalculate embeddings...")
    pred_emb = generate_embedding.generate_embedding(pred_text)
    target_emb = generate_embedding.generate_embedding(target_text)
    assert pred_emb.shape == target_emb.shape, "[ERROR] Predicted text embeddings' shape should match Target text embeddings' shape"

    # Send them to device
    pred_emb = torch.tensor(pred_emb, device = device)
    target_emb = torch.tensor(target_emb, device = device)

    # List for results
    result_list: List[Dict] = []
    for _ in range(rand_grp):

        # Generate index of rand_btsz of samples
        total_rows = pred_emb.shape[0]
        shuffled_indices = torch.randperm(total_rows)
        random_indices = shuffled_indices[:rand_btsz]

        # Select rows of pred_emb and target_emb
        selected_pred_emb = pred_emb[random_indices].clone().to(device)
        selected_target_emb = target_emb[random_indices].clone().to(device)

        # Assign first to be GT
        selected_pred_emb[0] = selected_target_emb[0]

        # 1. Normalize for cosine similarity
        pred_emb = torch.nn.functional.normalize(selected_pred_emb, p=2, dim=1)
        target_emb = torch.nn.functional.normalize(selected_target_emb, p=2, dim=1)

        # 2. Calculate Similarity Matrix (N x M)
        scores = torch.mm(selected_pred_emb, selected_target_emb.transpose(0, 1))

        # 3. Get Top-K indices for the largest K in the list
        ks = top_k
        max_k = max(ks)
        _, topk_indices = scores.topk(max_k, dim=1)  # (N, max_k)

        # 4. Check if ground_truth index is in the top-k retrieved indices
        # We expand ground_truth to (N, max_k) to compare directly
        # - Ground Truth: Index i in queries should match index i in gallery
        # (Assuming queries[i] matches gallery[i])
        ground_truth = torch.arange(selected_pred_emb.size(0), device = device).view(-1, 1)
        correct = topk_indices.eq(ground_truth.view(-1, 1).expand_as(topk_indices))

        results = { }
        for k in ks:
            # Sum correct matches within the first k columns
            correct_k = correct[:, :k].reshape(-1).float().sum(0)
            acc = float(correct_k / selected_pred_emb.size(0))
            results[f"retrieval_acc_top{k:02d}"] = acc

        # Append to list
        result_list.append(results)

    # Take Average
    results_to_return = { }
    keys = list(result_list[0].keys())
    for key in keys:
        accumulative = 0.0
        # Some every batch (low efficiency, but EASY coding)
        for i in range(rand_grp):
            accumulative += result_list[i][key]
        accumulative /= float(rand_grp)
        # Log that to the return dict
        results_to_return[key] = accumulative

    # Print summary metrics
    print("\n" + "=" * 60)
    print("Text Generation Retrieval Summary")
    print("=" * 60)
    for key in list(results_to_return.keys()):
        print(f"{key}:   {results_to_return[key]:.4f}")
    print("=" * 60)
    
    return results_to_return


# Open PATH_TO_STAGE2_DATAFRAME
stage2_df: pd.DataFrame = pd.read_pickle(PATH_TO_STAGE2_DATAFRAME)
if SET != 'all':
    # pick the target set
    index = stage2_df.index[stage2_df['phase'] == SET]
    stage2_df = stage2_df.iloc[index]

# Convert to prediction dict -> low efficiency but EASY coding
inputs = stage2_df['input text'].tolist()
preds = stage2_df['text_pred'].tolist()
assert len(inputs) == len(preds), "[ERROR] 'input text' column should have the same length as 'text_pred' column"
prediction_dict_list = [ ]
for input, pred in zip(inputs, preds):
    prediction_dict_list.append({'target' : input, 'prediction' : pred})

# Calculate metrics
compute_metrics(prediction_dict_list)
compute_retrieval_metrics(prediction_dict_list, device = DEVICE)